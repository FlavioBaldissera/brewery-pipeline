# ğŸº Brewery Data Pipeline with Apache Airflow

This project implements a complete **ETL pipeline** using **Apache Airflow** orchestrated via **Docker**. It extracts brewery data from a public API, processes and validates the data, stores it in a structured format (Parquet), and organizes the flow into **Bronze**, **Silver**, and **Gold** layers following the **Medallion Architecture**.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ dags/                      # Airflow DAGs
â”‚   â””â”€â”€ brewery_etl_dag.py     # Main DAG orchestrating the pipeline
â”œâ”€â”€ etl/                       # Python functions for each ETL step
â”‚   â”œâ”€â”€ extract.py             # Extract breweries from API
â”‚   â”œâ”€â”€ transform.py           # Clean, normalize, and partition data
â”œâ”€â”€ tests/                     # Unit tests for validation
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â””â”€â”€ test_transform.py
â”œâ”€â”€ data_lake/                 # Persisted data lake
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ docker-compose.yaml        # Docker stack definition
â”œâ”€â”€ start.sh                   # Startup script for Linux/macOS
â”œâ”€â”€ start.bat                  # Startup script for Windows
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1. Clone the repository

```bash
git clone https://github.com/FlavioBaldissera/brewery-pipeline
cd brewery-pipeline
```

---

### 2. Run the pipeline

#### ğŸªŸ For Windows

```cmd
start.bat
```

#### ğŸ§ For Linux/macOS

```bash
chmod +x start.sh   # only the first time
./start.sh
```

These scripts will:

- Build Docker images  
- Initialize the Airflow metadata DB and admin user  
- Start all required Airflow services  
- Wait for Airflow to become ready  
- Automatically **unpause** and **trigger** the DAG `brewery_etl`

Once running, access the Airflow UI at:  
ğŸ‘‰ **http://localhost:8080**

---

## ğŸ‘¤ Admin Login

The default admin user created is:

- **Username:** `admin`  
- **Password:** `admin`  

You can customize credentials by editing environment variables in the `.env` file or the scripts.

---

## ğŸ“… Running the DAG manually (optional)

If needed (e.g., for re-runs), you can trigger the DAG again from:

### The Airflow UI
1. Visit `http://localhost:8080`
2. Locate the `brewery_etl` DAG
3. Click â–¶ï¸ **Trigger DAG**

### Or from the CLI

```bash
docker-compose exec airflow-webserver airflow dags trigger brewery_etl
```

This will execute:

- `extract_breweries` â†’ stores raw JSON in **Bronze**
- `transform_breweries` â†’ cleans and partitions to Parquet by `state` in **Silver**
- `transform_breweries_gold` â†’ aggregates and stores summary as Parquet in **Gold**

---


## ğŸ§Š Data Lake Design

The ETL flow writes files to `data_lake/`, structured by layer:

```
data_lake/
â”œâ”€â”€ bronze/
â”‚   â””â”€â”€ breweries_YYYYMMDD.json
â”œâ”€â”€ silver/
â”‚   â””â”€â”€ breweries_YYYYMMDD.parquet/
â”‚       â”œâ”€â”€ state=Texas/breweries.parquet
â”‚       â”œâ”€â”€ state=Oklahoma/breweries.parquet
â”‚       â””â”€â”€ ...
â””â”€â”€ gold/
    â””â”€â”€ breweries_summary_YYYYMMDD.parquet
```

Partitioning by `state` is done to optimize analytical queries and performance.

---

## ğŸ”” Monitoring & Alerting

Suggested monitoring strategies:

- **Airflow UI:** Tracks retries, failures, execution duration
- **Email alerts:** Can be configured with Airflow's SMTP settings
- **Logging:** All tasks are logged for observability
- **Unit tests:** Prevent writing invalid/incomplete data
- **Future tools:** Integrate with tools like:
    - **Great Expectations** (data quality validation)
    - **Prometheus + Grafana** (metrics/alerts)
    - **Sentry** or **DataDog** (exception monitoring)

---

## ğŸ§¼ Cleaning Up

To stop and remove containers, networks, and volumes:

```bash
docker-compose down -v
```

To rebuild everything from scratch (use with caution):

```bash
docker-compose down --volumes
docker-compose up --build -d
```

---

## âœ… Requirements

- Docker & Docker Compose
- Internet access (to call external API)
- Python 3.10 (inside containers)

---

## ğŸ“ Notes

- All DAG logic is orchestrated using `brewery_etl_dag.py`
- External scripts are organized under the `etl/` folder
- DAGs and helper functions are imported dynamically using Pythonâ€™s module system
- Volumes are mapped so local files are persisted and visible under `data_lake/`

---